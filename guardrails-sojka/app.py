"""
Bielik Guard - System analizy bezpieczeÅ„stwa tekstu w jÄ™zyku polskim
Model klasyfikuje tekst pod kÄ…tem potencjalnie niebezpiecznych treÅ›ci
"""

# Importy z biblioteki transformers (Hugging Face) do obsÅ‚ugi modeli NLP
from transformers import pipeline  # Pipeline upraszcza interfejs do modeli
from huggingface_hub import login  # Autoryzacja w Hugging Face Hub

# Logowanie do Hugging Face Hub (wykorzystuje token z cache jeÅ›li istnieje)
# Potrzebne do pobrania modelu z repozytorium HF
login(new_session=False)

# ÅšcieÅ¼ka do modelu Bielik Guard - polskiego modelu klasyfikacji bezpieczeÅ„stwa tekstu
# Model jest hosowany na Hugging Face i zostanie automatycznie pobrany przy pierwszym uÅ¼yciu
model_path = "speakleash/Bielik-Guard-0.1B-v1.0" 

# Inicjalizacja pipeline dla klasyfikacji tekstu
# return_all_scores=True -> zwraca prawdopodobieÅ„stwa dla WSZYSTKICH kategorii, nie tylko najwyÅ¼szej
# Pipeline automatycznie obsÅ‚uguje: tokenizacjÄ™, inference, dekodowanie wynikÃ³w
classifier = pipeline("text-classification", model=model_path, return_all_scores=True)

def analyze_text(text):
    """Analizuje tekst i zwraca wyniki bezpieczeÅ„stwa"""
    
    # WywoÅ‚anie modelu - tekst przechodzi przez:
    # 1. TokenizacjÄ™ (tekst -> tokeny zrozumiaÅ‚e dla modelu)
    # 2. Forward pass przez sieÄ‡ neuronowÄ…
    # 3. Softmax -> prawdopodobieÅ„stwa dla kaÅ¼dej kategorii
    results = classifier(text)
    
    # Pipeline zwraca listÄ™ wynikÃ³w (nawet dla pojedynczego tekstu), bierzemy pierwszy element
    results = results[0]
    print(f"\nğŸ” Analizowany tekst: {text}")
    print("ğŸ“Š Oceny bezpieczeÅ„stwa:")
    
    # Iteracja przez wszystkie kategorie klasyfikacji
    # Model zwraca score (0-1) dla kaÅ¼dej z kategorii: self-harm, hate, vulgar, sex, crime
    for result in results:
        label = result['label']  # Nazwa kategorii z modelu
        score = result['score']   # PrawdopodobieÅ„stwo (confidence) dla danej kategorii

        # Mapowanie kategorii na emoji dla wizualizacji
        if 'self-harm' in label.lower():
            emoji = "ğŸ©¸"
        elif 'hate' in label.lower():
            emoji = "ğŸ¤"
        elif 'vulgar' in label.lower():
            emoji = "ğŸ¤¬"
        elif 'sex' in label.lower():
            emoji = "ğŸ’”"
        elif 'crime' in label.lower():
            emoji = "ğŸš”"
        else:
            emoji = "ğŸ“‹"
        
        print(f"  {emoji} {label}: {score:.3f}")
    
    # Wyznaczanie dominujÄ…cej kategorii (z najwyÅ¼szym score)
    # To kategoria, do ktÃ³rej model ma najwiÄ™kszÄ… pewnoÅ›Ä‡ (confidence)
    max_score = max(result['score'] for result in results)
    max_label = max(results, key=lambda x: x['score'])['label']
    
    print(f"\nğŸ¯ GÅ‚Ã³wna kategoria: {max_label}")
    print(f"ğŸ“ˆ OgÃ³lne ryzyko bezpieczeÅ„stwa: {max_score:.3f}")
    
    # Interpretacja wyniku na podstawie progÃ³w pewnoÅ›ci modelu
    # Progi sÄ… arbitralne i moÅ¼na je dostosowaÄ‡ do konkretnego use case
    if max_score > 0.7:
        print("ğŸš¨ WYSOKIE RYZYKO - tekst moÅ¼e zawieraÄ‡ niebezpieczne treÅ›ci")
    elif max_score > 0.4:
        print("ğŸ”” ÅšREDNIE RYZYKO - tekst wymaga uwagi")
    else:
        print("âœ… NISKIE RYZYKO - tekst wydaje siÄ™ bezpieczny")
    print("=" * 60)

def run_sample_tests():
    """Uruchamia testy na przykÅ‚adowych tekstach"""
    print("ğŸ§ª Uruchamianie testÃ³w na przykÅ‚adowych tekstach...")
    
    # ZbiÃ³r testowy reprezentujÄ…cy rÃ³Å¼ne kategorie treÅ›ci niebezpiecznych
    # KaÅ¼dy tekst powinien aktywowaÄ‡ innÄ… kategoriÄ™ modelu
    sample_texts = [
        "To jest normalna wiadomoÅ›Ä‡ bez problemÃ³w.",  # Neutralny baseline
        "ChcÄ™ siÄ™ zabiÄ‡ i skoÅ„czyÄ‡ z tym wszystkim.",  # Self-harm
        "Dlaczego hipisi sÄ… tacy brudni i gÅ‚upi",      # Hate speech
        "K*rwa, to jest ch*jowe rozwiÄ…zanie.",         # Vulgar language
        "Opowiem ci o seksie i intymnoÅ›ci.",           # Sexual content
        "Jak ukraÅ›Ä‡ samochÃ³d i nie zostaÄ‡ zÅ‚apanym."   # Crime-related
    ]
    
    for text in sample_texts:
        analyze_text(text)
        print("-" * 60)

def interactive_mode():
    """
    Tryb interaktywny do analizy tekstu w czasie rzeczywistym
    KaÅ¼de wprowadzone zdanie przechodzi przez peÅ‚ny pipeline klasyfikacji
    """
    print("WprowadÅº tekst do analizy (wpisz 'quit' aby wyjÅ›Ä‡, 'test' dla przykÅ‚adÃ³w)")
    
    while True:
        try:
            user_input = input("\nğŸ“ WprowadÅº tekst: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'wyjÅ›cie', 'q']:
                print("ğŸ‘‹ DziÄ™kujÄ™ za korzystanie z Bielik Guard!")
                break
            elif user_input.lower() in ['test', 'testy', 'przykÅ‚ady']:
                run_sample_tests()
                continue
            elif not user_input:
                print("âš ï¸ ProszÄ™ wprowadziÄ‡ jakiÅ› tekst.")
                continue
            
            analyze_text(user_input)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ DziÄ™kujÄ™ za korzystanie z Bielik Guard!")
            break
        except Exception as e:
            print(f"âŒ WystÄ…piÅ‚ bÅ‚Ä…d: {e}")

if __name__ == "__main__":
    # Punkt wejÅ›cia aplikacji - wykonuje siÄ™ tylko przy bezpoÅ›rednim uruchomieniu skryptu
    print("ğŸ¦ SÃ³jka - System analizy bezpieczeÅ„stwa tekstu")
    print("ğŸ›¡ï¸ Model: speakleash/Bielik-Guard-0.1B-v1.0")
    print("=" * 60)
    
    # Uruchomienie trybu interaktywnego - uÅ¼ytkownik moÅ¼e wprowadzaÄ‡ wÅ‚asne teksty do analizy
    # Model jest juÅ¼ zaÅ‚adowany w pamiÄ™ci (zainicjalizowany na poczÄ…tku skryptu)
    # Kolejne wywoÅ‚ania wykorzystujÄ… ten sam model bez ponownego Å‚adowania
    interactive_mode()